{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5a1437-48ab-43c2-8698-657fde21bfe3",
   "metadata": {},
   "source": [
    "# Set Up the Hazelcast Connection\n",
    "The documentation for the Hazelcast Python client is here: https://hazelcast.readthedocs.io/en/latest/\n",
    "\n",
    "The first thing we will do is establish a connection to the cluster, retrieve the \"images\" vector collection, and print out it's size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df52bff6-f8a9-401b-81aa-f8395dc7e853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hazelcast\n",
    "\n",
    "# connect to Hazelcast\n",
    "hz = hazelcast.HazelcastClient(cluster_name = \"dev\", cluster_members = [\"hz:5701\"])\n",
    "\n",
    "# retrieve the vector collection and print the size\n",
    "vectors = hz.get_vector_collection('images').blocking()\n",
    "vectors.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e41c9-90c6-4898-ad5b-c99ccd8fd4fc",
   "metadata": {},
   "source": [
    "# Set Up the Embedding \n",
    "\n",
    "A vector collection only deals with vectors.  In order to do a vector similarity search, we need a query vector. This is obtained by applying the same embedding to the query as we did to the images.  In this case, we are using the sentence_transformers package and specifically, the \"clip-ViT-B-32\" model.  \n",
    "\n",
    "The SentenceTransformer performs the embedding.  It can be initialized using just a model name, in which case it will be downloaded from the huggingface model repository.  We have already downloaded the model, so, to speed things up, we will load it from the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a87301-5d8f-48e7-93ab-49cb0ce3c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the sentence transformer \n",
    "import sentence_transformers\n",
    "encoder = sentence_transformers.SentenceTransformer('/project/models/clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cfce6a-2237-428f-b193-6aacdcbd2af4",
   "metadata": {},
   "source": [
    "# Run the Query\n",
    "\n",
    "This is a 2 step process.  First we transform the input query into a vector, then we call the Hazelcast \n",
    "\"search_near_vector\" API to find find similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d21e9974-9135-41c8-b4b4-c0d78585910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://localhost:8000/mayfly_0008.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.6422616839408875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://localhost:8000/mayfly_0019.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.6386593580245972\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://localhost:8000/mayfly_0002.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0.6339011192321777\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "from hazelcast.vector import Vector, VectorType\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Embed the Query\n",
    "#\n",
    "# we turn the query, which is text, into a vector using the previously initialized sentence transformer \n",
    "# \n",
    "query = encoder.encode(\"dragonfly\")\n",
    "\n",
    "# Create a hazelcast.vector.Vector instance representing the search\n",
    "# \n",
    "# We create a search vector object using hazelcast.vector.Vector\n",
    "# in additionn to the vector itself (array of numbers), we also pass the name of the index we wish to search\n",
    "# and the vector type.  \n",
    "#\n",
    "# The encoder returns a numpy array but we need to pass a list of floats to the vector API\n",
    "# We call \"tolist()\" to perform the conversion\n",
    "#\n",
    "search_vector = Vector(name='semantic-search', type = VectorType.DENSE, vector=query.tolist())\n",
    "\n",
    "# Search the vector collection\n",
    "# \n",
    "# now we can run the search, passing the previously created Vector object \n",
    "# we can also specify what we want the query to return (e.g. the actual vector, the vector metadata)\n",
    "# in our case, the vector \"value\" is the URL of the matching image so we pass \"include_value=True\"\n",
    "#\n",
    "results = vectors.search_near_vector(search_vector,include_value=True, limit=3)\n",
    "\n",
    "# Display the results \n",
    "for result in results:\n",
    "    url = result.value\n",
    "    # the image will actually be retrieved by the browser, which is outside of the Docker environment \n",
    "    # so we change \"www\" to \"localhost\"\n",
    "    url = url.replace('www','localhost')\n",
    "    display(Image(url=url))\n",
    "    print(f'score={result.score}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e1442-2e76-474e-9838-38592118e214",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "You may have noticed that the images don't always match the query.  The similarity search just finds the nearest vectors.  If you ask for \"Zebras\" and there are no zebras, the code above will just return the 3 closest images.  The score is actually the proximity (using cosine similarity) of the image to the query vector.  \n",
    "\n",
    "There is no universal definition of distance that is close enough to indicate a match.  Additionally, the distances do not cover a very wide range, typically .55 to .70 or so.  However, through trial and error, it appears that a threshold of .63 does a decent job of discriminating matches from non-matches.  \n",
    "\n",
    "Rework the example above to only display images that have a .63 or higher similarity score.\n",
    "\n",
    "Note: despite it sometimes being called a \"distance\", the cosine metric varies from 0 to 1 with 1 being the closest.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
